{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c1fb451",
   "metadata": {},
   "source": [
    "<p style=\"color:red\">\n",
    "NOTEBOOK LAST EDITED ON: 20230625\n",
    "</p>\n",
    "\n",
    "---\n",
    "# SLT Alignment Summit - RLCT Estimation Demo\n",
    "---\n",
    "\n",
    "In this notebook, we document a method used to estimate RLCT following Theorem 4 of the WBIC paper: \n",
    "```\n",
    "Watanabe, Sumio. 2013. “A Widely Applicable Bayesian Information Criterion.” Journal of Machine Learning Research: JMLR 14 (Mar): 867–97.\n",
    "```\n",
    "\n",
    "# Brief theory\n",
    "\n",
    "Given a model $p(x \\mid w)$, prior $\\varphi(w)$ and dataset $D_n = \\{X_i: i = 1, \\dots, n\\}$ drawn i.i.d. from a true distribution $q(x)$, the theorem assert that for an empirical negative log-likelihood function $L_n(w) = -\\frac{1}{n} \\sum_i \\log p(X_i \\mid w)$ satisfies the following asymptotic expansion in $n$\n",
    "$$\n",
    "\\mathbb{E}_{w}^\\beta[n L_n(w)] = n L_n(w_0) + \\frac{\\lambda}{\\beta} + U_n \\sqrt{\\frac{\\lambda}{\\beta}} + O_p(1)\n",
    "$$\n",
    "where \n",
    "  * $\\mathbb{E}_w^{\\beta}[]$ is expectation over the tempered posterior distribution with inverse temperature $\\beta$, \n",
    "    $$\n",
    "    p(w \\mid D_n) \\propto \\varphi(w) \\prod_{i} p(X_i \\mid w)^\\beta = \\varphi(w) e^{-\\beta n L_n(w)}\n",
    "    $$\n",
    "  * the inverse temperature is chosen at $\\beta = \\beta_0 / \\log n$ for some constant $\\beta_0$. \n",
    "  * $w_0$ is global minima of the averaged log-likelihood function $L(w) = -\\int q(x) \\log p(x \\mid w) dx$. \n",
    "  * $\\lambda$ is the learning coefficient or the RLCT of the zero set of $L(w) - L(w_0)$. \n",
    "\n",
    "Hence, for large enough $n$, we expect linear regression for a set of points $(1/ \\beta, \\mathbb{E}_{w}^\\beta[n L_n(w)])$ would result with intercept and slope that could estimate $nL_n(w_0)$ and $\\lambda$. \n",
    "\n",
    "Intuitively, having higher temperature (lower $\\beta$) changes the scale of the probability mass around the optimal parameter set $W_0$, or put differently, the region of high probability which concentrates near $W_0$ becomes larger as temperature is raised. The higher the temperature, the more we accept \"far away\" subpar solution included during Bayesian averaging, i.e. the larger the volume of high likelihood region. The the model is regular, this change in volume scale as the dimension $d$ of the parameter space. But in singular models, this is instead controlled by the RLCT $2 * \\lambda$. By observing the tempered posterior average of $nL_n(w)$ at a range of temperature, we probing this scaling relation. \n",
    "\n",
    "\n",
    "# Implementation\n",
    "We will implement the probability model and MCMC sampling in [numpyro](https://num.pyro.ai/en/stable/index.html). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d53042",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289103c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf3f16f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0982ca81",
   "metadata": {},
   "source": [
    "----\n",
    "# Regular Models\n",
    "----\n",
    "## 1D Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9e24c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a3941e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "098838ed",
   "metadata": {},
   "source": [
    "## Higher-D Linear Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce76658",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b00edbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0722412e",
   "metadata": {},
   "source": [
    "---\n",
    "# Singular Models\n",
    "---\n",
    "## Truth Singular for the Model\n",
    " - `tanh` network. \n",
    " - `reduced rank regression`\n",
    " - Toy model of superposition.  \n",
    "\n",
    "\n",
    "## Truth Regular for the Model\n",
    "\n",
    "\n",
    "\n",
    "## Non-analytic machines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c435c9f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
